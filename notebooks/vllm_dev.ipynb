{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc737598-0f75-406d-9e02-4e60d6ccb538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"WSDM-Cup-Multilingual-Chatbot-Arena-Kaggle/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482c0182-22ca-4cf7-a832-a2cbfaf53141",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"Qwen/Qwen2-7B-Instruct\", # pass\n",
    "    # \"microsoft/Phi-3-small-8k-instruct\", # fail (unpack error)\n",
    "    \"01-ai/Yi-1.5-9B-Chat\", # pass\n",
    "    \"Qwen/Qwen1.5-14B-Chat-AWQ\", # pass\n",
    "    # \"meta-llama/Llama-3.2-1B\", # fail (chat template issue)\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", # pass\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", # pass\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\", # pass\n",
    "    # \"alpaca-13b\", # fail\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\", # pass\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\", # pass\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", # pass\n",
    "    \"HuggingFaceTB/SmolLM-1.7B-Instruct\", # pass\n",
    "    \"google/gemma-2-2b-it\", # pass\n",
    "    \"google/gemma-2-9b-it\", # pass\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\", # pass\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\", # pass\n",
    "    \"Qwen/Qwen2.5-14B-Instruct-AWQ\", # pass\n",
    "    'Nexusflow/Starling-LM-7B-beta', # to check\n",
    "    'Qwen/Qwen1.5-14B-Chat-AWQ', # to check\n",
    "    'meta-llama/Meta-Llama-3-8B-Instruct', # to check\n",
    "    # \"Qwen/Qwen2.5-32B-Instruct-AWQ\" # see if A100 can run this, OOM on A5000\n",
    "    # \"google/gemma-2-27b-it\", # see if A100 can run this, OOM on A5000\n",
    "]\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa675ea-2f6a-4fc6-86a9-75c973c9494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "with open(\"vllm_inference_ids.pkl\", \"rb\") as f:\n",
    "    ignore_ids = pickle.load(f)\n",
    "\n",
    "ds = load_dataset(\"lmsys/lmsys-chat-1m\", split=\"train\").select(range(10_000))\n",
    "ds = ds.filter(lambda example: example[\"language\"] != \"English\", num_proc=8)\n",
    "ds = ds.filter(lambda example: example[\"turn\"] == 1, num_proc=8)\n",
    "ds = ds.filter(lambda x: x[\"conversation_id\"] not in ignore_ids, batch_size=1000,num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ae9916-f444-4ac0-a6b1-af35af3e4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_model(example):\n",
    "    return {'gen_model': random.choice(models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7aa9556-54a2-4bd7-810e-c78b0d85dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(select_model, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c21b37-33c8-47ef-8b2a-177daa8449c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(example):\n",
    "    return {\"prompt\": next(msg[\"content\"] for msg in example[\"conversation\"] if msg[\"role\"] == \"user\")}\n",
    "ds = ds.map(get_prompt, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf84d1e-ad29-4aea-b806-2876b018976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Qwen/Qwen2-7B-Instruct\n",
      "INFO 01-08 06:32:18 config.py:510] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-08 06:32:18 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-08 06:32:20 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-08 06:32:20 model_runner.py:1094] Starting to load model Qwen/Qwen2-7B-Instruct...\n",
      "INFO 01-08 06:32:21 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ce0f0e529f4fc88f94d8f226942bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:33:03 model_runner.py:1099] Loading model weights took 14.2487 GB\n",
      "INFO 01-08 06:33:05 worker.py:241] Memory profiling takes 1.72 seconds\n",
      "INFO 01-08 06:33:05 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.95) = 22.50GiB\n",
      "INFO 01-08 06:33:05 worker.py:241] model weights take 14.25GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 6.71GiB.\n",
      "INFO 01-08 06:33:05 gpu_executor.py:76] # GPU blocks: 7852, # CPU blocks: 4681\n",
      "INFO 01-08 06:33:05 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 15.34x\n",
      "INFO 01-08 06:33:10 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:33:26 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.20 GiB\n",
      "INFO 01-08 06:33:26 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.52 seconds\n",
      "Processing 65 samples for Qwen/Qwen2-7B-Instruct\n",
      "INFO 01-08 06:33:26 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 65/65 [00:44<00:00,  1.45it/s, est. speed input: 121.73 toks/s, output: 383.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 01-ai/Yi-1.5-9B-Chat\n",
      "INFO 01-08 06:34:19 config.py:510] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-08 06:34:19 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='01-ai/Yi-1.5-9B-Chat', speculative_config=None, tokenizer='01-ai/Yi-1.5-9B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=01-ai/Yi-1.5-9B-Chat, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-08 06:34:21 model_runner.py:1094] Starting to load model 01-ai/Yi-1.5-9B-Chat...\n",
      "INFO 01-08 06:34:21 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737f66f9f2dc4cf58ff823e8f645c15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:34:25 model_runner.py:1099] Loading model weights took 16.4470 GB\n",
      "INFO 01-08 06:34:26 worker.py:241] Memory profiling takes 1.01 seconds\n",
      "INFO 01-08 06:34:26 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.95) = 22.50GiB\n",
      "INFO 01-08 06:34:26 worker.py:241] model weights take 16.45GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.61GiB; the rest of the memory reserved for KV Cache is 5.43GiB.\n",
      "INFO 01-08 06:34:26 gpu_executor.py:76] # GPU blocks: 3709, # CPU blocks: 2730\n",
      "INFO 01-08 06:34:26 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 14.49x\n",
      "INFO 01-08 06:34:32 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:34:49 model_runner.py:1535] Graph capturing finished in 17 secs, took 0.14 GiB\n",
      "INFO 01-08 06:34:49 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 24.11 seconds\n",
      "Processing 52 samples for 01-ai/Yi-1.5-9B-Chat\n",
      "INFO 01-08 06:34:49 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 52/52 [01:32<00:00,  1.77s/it, est. speed input: 111.91 toks/s, output: 329.71 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Qwen/Qwen1.5-14B-Chat-AWQ\n",
      "INFO 01-08 06:36:24 config.py:510] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-08 06:36:24 awq_marlin.py:113] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 01-08 06:36:24 config.py:588] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 01-08 06:36:24 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen1.5-14B-Chat-AWQ', speculative_config=None, tokenizer='Qwen/Qwen1.5-14B-Chat-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen1.5-14B-Chat-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-08 06:36:25 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-08 06:36:25 model_runner.py:1094] Starting to load model Qwen/Qwen1.5-14B-Chat-AWQ...\n",
      "INFO 01-08 06:36:26 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889de3ecfdfc4d2da59ee5a26f289594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:36:28 model_runner.py:1099] Loading model weights took 9.0681 GB\n",
      "INFO 01-08 06:36:31 worker.py:241] Memory profiling takes 3.07 seconds\n",
      "INFO 01-08 06:36:31 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.95) = 22.50GiB\n",
      "INFO 01-08 06:36:31 worker.py:241] model weights take 9.07GiB; non_torch_memory takes 0.01GiB; PyTorch activation peak memory takes 1.46GiB; the rest of the memory reserved for KV Cache is 11.96GiB.\n",
      "INFO 01-08 06:36:32 gpu_executor.py:76] # GPU blocks: 980, # CPU blocks: 327\n",
      "INFO 01-08 06:36:32 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 1.91x\n",
      "INFO 01-08 06:36:32 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:36:54 model_runner.py:1535] Graph capturing finished in 23 secs, took 0.48 GiB\n",
      "INFO 01-08 06:36:54 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 26.04 seconds\n",
      "Processing 56 samples for Qwen/Qwen1.5-14B-Chat-AWQ\n",
      "INFO 01-08 06:36:54 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 56/56 [00:41<00:00,  1.35it/s, est. speed input: 125.95 toks/s, output: 342.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: meta-llama/Llama-3.2-3B-Instruct\n",
      "INFO 01-08 06:37:40 config.py:510] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-08 06:37:40 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-08 06:37:41 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 01-08 06:37:41 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f389227b8c9a4e3b9f36eb5fa9ce61ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:37:43 model_runner.py:1099] Loading model weights took 6.0160 GB\n",
      "INFO 01-08 06:37:44 worker.py:241] Memory profiling takes 0.79 seconds\n",
      "INFO 01-08 06:37:44 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.95) = 22.50GiB\n",
      "INFO 01-08 06:37:44 worker.py:241] model weights take 6.02GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 15.24GiB.\n",
      "INFO 01-08 06:37:44 gpu_executor.py:76] # GPU blocks: 8917, # CPU blocks: 2340\n",
      "INFO 01-08 06:37:44 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 17.42x\n",
      "INFO 01-08 06:37:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-08 06:37:59 model_runner.py:1535] Graph capturing finished in 15 secs, took 0.05 GiB\n",
      "INFO 01-08 06:37:59 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 15.72 seconds\n",
      "Processing 57 samples for meta-llama/Llama-3.2-3B-Instruct\n",
      "INFO 01-08 06:37:59 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 57/57 [00:17<00:00,  3.24it/s, est. speed input: 175.30 toks/s, output: 705.74 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: meta-llama/Llama-3.1-8B-Instruct\n",
      "INFO 01-08 06:38:20 config.py:510] This model supports multiple tasks: {'classify', 'score', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-08 06:38:20 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-08 06:38:21 model_runner.py:1094] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 01-08 06:38:22 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eefc314e5964ab8b315c1a738d99716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    all_ds = []\n",
    "    for model in models:\n",
    "        # load the vllm instance here\n",
    "\n",
    "        max_model_len = 8192\n",
    "        if model in  [\"01-ai/Yi-1.5-9B-Chat\", \"microsoft/Phi-3-mini-4k-instruct\"] :\n",
    "            max_model_len = 4096\n",
    "        elif model == \"HuggingFaceTB/SmolLM-1.7B-Instruct\":\n",
    "            max_model_len = 2048\n",
    "        print(f\"model: {model}\")\n",
    "        if model == \"unsloth/gemma-2-27b-it-bnb-4bit\":\n",
    "            llm = LLM(\n",
    "            model=model,\n",
    "            dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            load_format=\"bitsandbytes\",\n",
    "            quantization=\"bitsandbytes\", \n",
    "            seed=0,\n",
    "            gpu_memory_utilization=0.95,\n",
    "            max_model_len=max_model_len)\n",
    "        else:    \n",
    "            llm = LLM(\n",
    "                model=model,\n",
    "                dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                quantization=\"awq\" if \"awq\" in model.lower() else None, \n",
    "                seed=0,\n",
    "                gpu_memory_utilization=0.95,\n",
    "                max_model_len=max_model_len\n",
    "            )\n",
    "        sampling_params = SamplingParams(n=1, temperature=0.7, max_tokens=4096, seed=0, top_p=0.9)\n",
    "    \n",
    "        # filter on gen_model\n",
    "        model_ds = ds.filter(lambda x: x[\"gen_model\"] == model, num_proc=8)\n",
    "        print(f\"Processing {len(model_ds)} samples for {model}\")\n",
    "    \n",
    "        conversations = []\n",
    "        for d in model_ds:\n",
    "            conversations.append([\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": d[\"prompt\"]\n",
    "            },\n",
    "        ])\n",
    "    \n",
    "        outputs = llm.chat(conversations, sampling_params=sampling_params)\n",
    "        responses = []\n",
    "        for o in outputs:\n",
    "            response = o.outputs[0].text.strip()\n",
    "            responses.append(response)\n",
    "        model_ds = model_ds.add_column(name=\"generated_response\", column=responses)\n",
    "        all_ds.append(model_ds)\n",
    "    \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab8c41-c3ee-4d41-a629-57e260ba1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "final_ds = concatenate_datasets(all_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2dcef-70e4-4327-85de-ff062a236c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = []\n",
    "for f in final_ds:\n",
    "    # Randomly decide which response goes to A and B\n",
    "    orig_response = next(msg[\"content\"] for msg in f[\"conversation\"] if msg[\"role\"] == \"assistant\")\n",
    "    if random.random() < 0.5:\n",
    "        response_a = f[\"generated_response\"]\n",
    "        response_b = orig_response\n",
    "        model_a = f[\"gen_model\"]\n",
    "        model_b = f[\"model\"]\n",
    "    else:\n",
    "        response_a = orig_response\n",
    "        response_b = f[\"generated_response\"]\n",
    "        model_a = f[\"model\"]\n",
    "        model_b = f[\"gen_model\"]\n",
    "        \n",
    "    final_df.append({\n",
    "        \"id\": f[\"conversation_id\"],\n",
    "        \"prompt\": f[\"prompt\"],\n",
    "        \"response_a\": response_a,\n",
    "        \"response_b\": response_b,\n",
    "        \"model_a\": model_a,\n",
    "        \"model_b\": model_b,\n",
    "        \"language\": f[\"language\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3196449-2b23-44f7-b22c-5409ae1b7b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df = pd.DataFrame(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a70a1-b89f-481b-a35e-c386162755b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39a0a7-f638-4d9a-89e0-0a2913dbe226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
