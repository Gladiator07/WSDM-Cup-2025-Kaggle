{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class SequenceProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        max_length: int = 1600,\n",
    "        truncation_side: str = \"left\",\n",
    "        padding_side: str = \"left\",\n",
    "        add_eos_token: bool = False,\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            # use_fast=True,\n",
    "            # trust_remote_code=True,\n",
    "            # from_slow=True,\n",
    "            # add_prefix_space=False,\n",
    "            padding_side=padding_side,\n",
    "            truncation_side=truncation_side,\n",
    "        )\n",
    "        self.tokenizer.add_eos_token = add_eos_token\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Template parts (only tokenize once during initialization)\n",
    "        self.templates = {\n",
    "            \"start\": self.tokenizer.encode(\"# Prompt\\n\", add_special_tokens=False),\n",
    "            \"response_a\": self.tokenizer.encode(\"\\n\\n# Response A\\n\", add_special_tokens=False),\n",
    "            \"response_b\": self.tokenizer.encode(\"\\n\\n# Response B\\n\", add_special_tokens=False),\n",
    "            \"question\": self.tokenizer.encode(\"\\n\\n# Which response is better?\", add_special_tokens=False),\n",
    "            \"ellipsis\": self.tokenizer.encode(\" [...] \", add_special_tokens=False),\n",
    "        }\n",
    "\n",
    "        # Calculate fixed template length\n",
    "        self.template_length = sum(len(tokens) for tokens in self.templates.values()) - len(self.templates[\"ellipsis\"])\n",
    "\n",
    "    def truncate_if_needed(self, tokens, max_tokens):\n",
    "        \"\"\"Truncate tokens if they exceed max_tokens by keeping start and end portions.\"\"\"\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return tokens\n",
    "\n",
    "        keep_tokens = (max_tokens - len(self.templates[\"ellipsis\"])) // 2\n",
    "        return tokens[:keep_tokens] + self.templates[\"ellipsis\"] + tokens[-keep_tokens:]\n",
    "\n",
    "    def tokenize(self, row, tta=False):\n",
    "        if tta:\n",
    "            prompt, response_a, response_b = row[\"prompt\"], row[\"response_b\"], row[\"response_a\"]\n",
    "        else:\n",
    "            prompt, response_a, response_b = row[\"prompt\"], row[\"response_a\"], row[\"response_b\"]\n",
    "\n",
    "        # Available tokens after accounting for template and special tokens\n",
    "        buffer_tokens = 3\n",
    "        available_tokens = self.max_length - self.template_length - buffer_tokens  # -1 for BOS token\n",
    "\n",
    "        # Tokenize all inputs at once\n",
    "        enc = self.tokenizer([prompt, response_a, response_b], add_special_tokens=False)[\"input_ids\"]\n",
    "        prompt_tokens, response_a_tokens, response_b_tokens = enc[0], enc[1], enc[2]\n",
    "\n",
    "        total_length = len(prompt_tokens) + len(response_a_tokens) + len(response_b_tokens)\n",
    "\n",
    "        # If total length is within limit, return without truncation\n",
    "        if total_length <= available_tokens:\n",
    "            final_sequence = (\n",
    "                [self.tokenizer.bos_token_id]\n",
    "                + self.templates[\"start\"]\n",
    "                + prompt_tokens\n",
    "                + self.templates[\"response_a\"]\n",
    "                + response_a_tokens\n",
    "                + self.templates[\"response_b\"]\n",
    "                + response_b_tokens\n",
    "                + self.templates[\"question\"]\n",
    "            )\n",
    "            if \"qwen\" in self.model_name.lower():\n",
    "                final_sequence.pop(0)\n",
    "            if self.tokenizer.add_eos_token:\n",
    "                final_sequence.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "            return {\"input_ids\": final_sequence, \"attention_mask\": [1] * len(final_sequence), \"length\": len(final_sequence)}\n",
    "\n",
    "        # Allocate tokens based on 20-40-40 split with dynamic adjustment\n",
    "        prompt_max = int(available_tokens * 0.2)  # Reserve 20% for prompt\n",
    "        response_max = int(available_tokens * 0.4)  # 40% each for responses\n",
    "\n",
    "        # If prompt needs less than its allocation, distribute the excess\n",
    "        prompt_needed = min(len(prompt_tokens), prompt_max)\n",
    "        excess_tokens = prompt_max - prompt_needed\n",
    "\n",
    "        # Add half of excess to each response's budget\n",
    "        response_a_max = response_max + excess_tokens // 2\n",
    "        response_b_max = response_max + excess_tokens - (excess_tokens // 2)  # Account for odd number\n",
    "\n",
    "        # Calculate actual token allocations\n",
    "        prompt_max_tokens = prompt_needed\n",
    "        response_a_max_tokens = min(len(response_a_tokens), response_a_max)\n",
    "        response_b_max_tokens = min(len(response_b_tokens), response_b_max)\n",
    "\n",
    "        # Truncate each section if needed\n",
    "        prompt_tokens = self.truncate_if_needed(prompt_tokens, prompt_max_tokens)\n",
    "        response_a_tokens = self.truncate_if_needed(response_a_tokens, response_a_max_tokens)\n",
    "        response_b_tokens = self.truncate_if_needed(response_b_tokens, response_b_max_tokens)\n",
    "\n",
    "        # Assemble final input\n",
    "        final_sequence = (\n",
    "            [self.tokenizer.bos_token_id]\n",
    "            + self.templates[\"start\"]\n",
    "            + prompt_tokens\n",
    "            + self.templates[\"response_a\"]\n",
    "            + response_a_tokens\n",
    "            + self.templates[\"response_b\"]\n",
    "            + response_b_tokens\n",
    "            + self.templates[\"question\"]\n",
    "        )\n",
    "        if \"qwen\" in self.model_name.lower():\n",
    "            final_sequence.pop(0)\n",
    "        if self.tokenizer.add_eos_token:\n",
    "            final_sequence.append(self.tokenizer.eos_token_id)\n",
    "        return {\"input_ids\": final_sequence, \"attention_mask\": [1] * len(final_sequence), \"length\": len(final_sequence)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SequenceProcessor(model_name=\"Gladiator/wsdm_cup_phi4_stage1_lora_700_merged\",max_length=4096, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "      <th>split</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010ed04b536f56ebe43eef1100c13906abea12bf9855...</td>\n",
       "      <td>You will be given a piece of news. Analyze it ...</td>\n",
       "      <td>Let's break down the news and analyze it accor...</td>\n",
       "      <td>```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>Russian</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00172aab8af10cc0648041c94a41eeab7d9caaea7717a3...</td>\n",
       "      <td>kısaca dopamin detoksu</td>\n",
       "      <td>Dopamin detoksu, beyninizin dopamin seviyeleri...</td>\n",
       "      <td>Dopamin detoksunun amacı, aşırı uyarılmaya ned...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>grok-2-2024-08-13</td>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00314ee979ffc9e4e4dd3716d02c401ba117d00640a3e1...</td>\n",
       "      <td>In Webgpu/WGSL, what are override variables?</td>\n",
       "      <td>In **WebGPU** using **WGSL** (WebGPU Shading L...</td>\n",
       "      <td>As of my last update in 2023, WebGPU and WGSL ...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>chatgpt-4o-latest-20240903</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>English</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003aa0a76eb58c06adce6e9db59ad1da73929a431f3f23...</td>\n",
       "      <td>nêu lý do chọn đây là đề tài nghiên cứu \"Đánh...</td>\n",
       "      <td>Dưới đây là một số lý do có thể được sử dụng đ...</td>\n",
       "      <td>Chọn đề tài \"Đánh giá tác động của Social Medi...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>llama-3.1-405b-instruct-bf16</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00448e4160ceb9578584293b6aee5d680c8e6dbbcd1d13...</td>\n",
       "      <td>package ex3_Excercise;\\n//버블정렬\\npublic class T...</td>\n",
       "      <td>```java\\npackage ex3_Excercise;\\n\\npublic clas...</td>\n",
       "      <td>```java\\npackage ex3_Excercise;\\n\\npublic clas...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>command-r-08-2024</td>\n",
       "      <td>English</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>ffc2a1a209a1053658645fc60dd29249392c7e0040ae5d...</td>\n",
       "      <td>Are there decent AI therapy options yet? If so...</td>\n",
       "      <td>As an AI, I cannot provide medical advice, and...</td>\n",
       "      <td>The fascinating intersection of AI and mental ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>llama-3.1-nemotron-70b-instruct</td>\n",
       "      <td>English</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>ffd929efef61e5b353180a6e790d35896363ecf02973ee...</td>\n",
       "      <td>Imagine you have to get two goats and a cabbag...</td>\n",
       "      <td>To solve the problem of getting two goats and ...</td>\n",
       "      <td>To solve this classic problem, we'll break it ...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-mini</td>\n",
       "      <td>llama-3.1-405b-instruct-bf16</td>\n",
       "      <td>English</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>ffe2645c0cdb2cbbb755ad0766cfc14663726619968c4e...</td>\n",
       "      <td>翻译一下这句话：Builder Journey Experience, 优化开发者在网站及各...</td>\n",
       "      <td>Builder Journey Experience optimizes developer...</td>\n",
       "      <td>Here is a translation of the sentence:\\n\\n**Bu...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>glm-4-plus</td>\n",
       "      <td>llama-3.1-nemotron-70b-instruct</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>ffe2a8b3cf149dc4ffd040623f4c8e4e9e87b98bd41b14...</td>\n",
       "      <td>Какое программное обеспечение развернуто на Уп...</td>\n",
       "      <td>На управляющих узлах OpenShift развернуто неск...</td>\n",
       "      <td>На Управляющих узлах (Master Nodes) OpenShift ...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-flash-8b-001</td>\n",
       "      <td>yi-lightning</td>\n",
       "      <td>unknown</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>fffd2ffdaa03e9e0a0cd1e8f2ee80f530bb19b08fa4312...</td>\n",
       "      <td>What is the height of the church located in Sa...</td>\n",
       "      <td>**Church Information: Église de Saint-Nazaire-...</td>\n",
       "      <td>I couldn't find any information about a church...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>llama-3.2-3b-instruct</td>\n",
       "      <td>English</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4839 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0     00010ed04b536f56ebe43eef1100c13906abea12bf9855...   \n",
       "1     00172aab8af10cc0648041c94a41eeab7d9caaea7717a3...   \n",
       "2     00314ee979ffc9e4e4dd3716d02c401ba117d00640a3e1...   \n",
       "3     003aa0a76eb58c06adce6e9db59ad1da73929a431f3f23...   \n",
       "4     00448e4160ceb9578584293b6aee5d680c8e6dbbcd1d13...   \n",
       "...                                                 ...   \n",
       "4834  ffc2a1a209a1053658645fc60dd29249392c7e0040ae5d...   \n",
       "4835  ffd929efef61e5b353180a6e790d35896363ecf02973ee...   \n",
       "4836  ffe2645c0cdb2cbbb755ad0766cfc14663726619968c4e...   \n",
       "4837  ffe2a8b3cf149dc4ffd040623f4c8e4e9e87b98bd41b14...   \n",
       "4838  fffd2ffdaa03e9e0a0cd1e8f2ee80f530bb19b08fa4312...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     You will be given a piece of news. Analyze it ...   \n",
       "1                               kısaca dopamin detoksu    \n",
       "2          In Webgpu/WGSL, what are override variables?   \n",
       "3      nêu lý do chọn đây là đề tài nghiên cứu \"Đánh...   \n",
       "4     package ex3_Excercise;\\n//버블정렬\\npublic class T...   \n",
       "...                                                 ...   \n",
       "4834  Are there decent AI therapy options yet? If so...   \n",
       "4835  Imagine you have to get two goats and a cabbag...   \n",
       "4836  翻译一下这句话：Builder Journey Experience, 优化开发者在网站及各...   \n",
       "4837  Какое программное обеспечение развернуто на Уп...   \n",
       "4838  What is the height of the church located in Sa...   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     Let's break down the news and analyze it accor...   \n",
       "1     Dopamin detoksu, beyninizin dopamin seviyeleri...   \n",
       "2     In **WebGPU** using **WGSL** (WebGPU Shading L...   \n",
       "3     Dưới đây là một số lý do có thể được sử dụng đ...   \n",
       "4     ```java\\npackage ex3_Excercise;\\n\\npublic clas...   \n",
       "...                                                 ...   \n",
       "4834  As an AI, I cannot provide medical advice, and...   \n",
       "4835  To solve the problem of getting two goats and ...   \n",
       "4836  Builder Journey Experience optimizes developer...   \n",
       "4837  На управляющих узлах OpenShift развернуто неск...   \n",
       "4838  **Church Information: Église de Saint-Nazaire-...   \n",
       "\n",
       "                                             response_b   winner  \\\n",
       "0     ```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...  model_a   \n",
       "1     Dopamin detoksunun amacı, aşırı uyarılmaya ned...  model_b   \n",
       "2     As of my last update in 2023, WebGPU and WGSL ...  model_a   \n",
       "3     Chọn đề tài \"Đánh giá tác động của Social Medi...  model_a   \n",
       "4     ```java\\npackage ex3_Excercise;\\n\\npublic clas...  model_a   \n",
       "...                                                 ...      ...   \n",
       "4834  The fascinating intersection of AI and mental ...  model_b   \n",
       "4835  To solve this classic problem, we'll break it ...  model_a   \n",
       "4836  Here is a translation of the sentence:\\n\\n**Bu...  model_b   \n",
       "4837  На Управляющих узлах (Master Nodes) OpenShift ...  model_a   \n",
       "4838  I couldn't find any information about a church...  model_a   \n",
       "\n",
       "                           model_a                          model_b  \\\n",
       "0                   gemma-2-27b-it             gemini-1.5-flash-002   \n",
       "1                grok-2-2024-08-13               gemini-1.5-pro-002   \n",
       "2       chatgpt-4o-latest-20240903               gpt-4-0125-preview   \n",
       "3     llama-3.1-405b-instruct-bf16               reka-core-20240904   \n",
       "4                    gemma-2-9b-it                command-r-08-2024   \n",
       "...                            ...                              ...   \n",
       "4834                 gemma-2-9b-it  llama-3.1-nemotron-70b-instruct   \n",
       "4835                       o1-mini     llama-3.1-405b-instruct-bf16   \n",
       "4836                    glm-4-plus  llama-3.1-nemotron-70b-instruct   \n",
       "4837       gemini-1.5-flash-8b-001                     yi-lightning   \n",
       "4838        llama-3.1-70b-instruct            llama-3.2-3b-instruct   \n",
       "\n",
       "        language  split        source  \n",
       "0        Russian  valid  current_comp  \n",
       "1        Finnish  valid  current_comp  \n",
       "2        English  valid  current_comp  \n",
       "3     Vietnamese  valid  current_comp  \n",
       "4        English  valid  current_comp  \n",
       "...          ...    ...           ...  \n",
       "4834     English  valid  current_comp  \n",
       "4835     English  valid  current_comp  \n",
       "4836     Chinese  valid  current_comp  \n",
       "4837     unknown  valid  current_comp  \n",
       "4838     English  valid  current_comp  \n",
       "\n",
       "[4839 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"../data/pseudo_labeled_final_data/train_stage2_stage1_combined_v1.parquet\")\n",
    "df = df[df['split'] == 'valid']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.apply(lambda x: processor.tokenize(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "      <th>split</th>\n",
       "      <th>source</th>\n",
       "      <th>gemma9b</th>\n",
       "      <th>gemma27b</th>\n",
       "      <th>phi4</th>\n",
       "      <th>soft_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vieš po Slovensky?</td>\n",
       "      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n",
       "      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Slovak</td>\n",
       "      <td>train</td>\n",
       "      <td>current_comp</td>\n",
       "      <td>[0.802734375, -0.159423828125]</td>\n",
       "      <td>[-0.728515625, -1.80859375]</td>\n",
       "      <td>[0.86328125, -0.49609375]</td>\n",
       "      <td>[0.985, 0.015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000c48f341d489591e3bbeca1821926</td>\n",
       "      <td>¿Cómo hago una petición Ajax en django con jQu...</td>\n",
       "      <td>Para hacer una petición Ajax en Django con jQu...</td>\n",
       "      <td>¡Hola! ¡Claro que sí! Aquí te puedo ayudar con...</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen/Qwen2-1.5B-Instruct</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>train</td>\n",
       "      <td>lmsys1m_vllm_gen</td>\n",
       "      <td>[-0.40869140625, 0.94140625]</td>\n",
       "      <td>[-1.5546875, -0.9609375]</td>\n",
       "      <td>[-0.35595703125, 0.783203125]</td>\n",
       "      <td>[0.2412032986690225, 0.7587967013309777]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000d826867d47dabd7d4b110beea2e9</td>\n",
       "      <td>You are now an expert educator. You have acces...</td>\n",
       "      <td>Cognitive bias is a tendency of the mind to fo...</td>\n",
       "      <td>Cognitive Bias is a phenomenon that occurs whe...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>fastchat-t5-3b</td>\n",
       "      <td>RWKV-4-Raven-14B</td>\n",
       "      <td>English</td>\n",
       "      <td>train</td>\n",
       "      <td>lmsys_33k</td>\n",
       "      <td>[0.3466033935546875, 0.2843017578125]</td>\n",
       "      <td>[-1.45703125, -0.95703125]</td>\n",
       "      <td>[0.173828125, 0.28857421875]</td>\n",
       "      <td>[0.985, 0.015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000f23b2f8049ac9d92c28d4817381e</td>\n",
       "      <td>Please act my sick old grandma who would read ...</td>\n",
       "      <td>Oh dear, it sounds like you're looking for a c...</td>\n",
       "      <td>Oh, my dear sweet child, come closer and snugg...</td>\n",
       "      <td>None</td>\n",
       "      <td>MiniMaxAI/MiniMax-Text-01</td>\n",
       "      <td>deepseek-ai/DeepSeek-V3</td>\n",
       "      <td>English</td>\n",
       "      <td>train</td>\n",
       "      <td>nicholas_broad_hf</td>\n",
       "      <td>[0.16259765625, 0.3935546875]</td>\n",
       "      <td>[-1.421875, -1.01953125]</td>\n",
       "      <td>[0.25439453125, 0.1134033203125]</td>\n",
       "      <td>[0.45704432372858816, 0.5429556762714118]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00010ed04b536f56ebe43eef1100c13906abea12bf9855...</td>\n",
       "      <td>You will be given a piece of news. Analyze it ...</td>\n",
       "      <td>Let's break down the news and analyze it accor...</td>\n",
       "      <td>```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>Russian</td>\n",
       "      <td>valid</td>\n",
       "      <td>current_comp</td>\n",
       "      <td>[1.048828125, -0.31005859375]</td>\n",
       "      <td>[-0.534423828125, -2.0859375]</td>\n",
       "      <td>[1.0, -0.515625]</td>\n",
       "      <td>[0.985, 0.015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180693</th>\n",
       "      <td>fffe4d219bf14e50a7f29f64ee8a2fa0</td>\n",
       "      <td>puoi organizzare meglio questo documento latex...</td>\n",
       "      <td>Certo, ecco il codice aggiornato con una maggi...</td>\n",
       "      <td>\\documentclass{article}\\n\\n\\usepackage[utf8]{i...</td>\n",
       "      <td>None</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
       "      <td>Italian</td>\n",
       "      <td>train</td>\n",
       "      <td>lmsys1m_vllm_gen</td>\n",
       "      <td>[-0.564453125, 1.25]</td>\n",
       "      <td>[-1.6484375, -0.80078125]</td>\n",
       "      <td>[-0.40625, 0.83984375]</td>\n",
       "      <td>[0.18557792051464522, 0.8144220794853547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180694</th>\n",
       "      <td>fffe559651f241bdb473eb396ccdf042</td>\n",
       "      <td>zxcvzxcv</td>\n",
       "      <td>It looks like you've typed a random string of ...</td>\n",
       "      <td>It seems like you've entered a string of rando...</td>\n",
       "      <td>None</td>\n",
       "      <td>MiniMaxAI/MiniMax-Text-01</td>\n",
       "      <td>meta-llama/Llama-3.1-405B-Instruct</td>\n",
       "      <td>Danish</td>\n",
       "      <td>train</td>\n",
       "      <td>nicholas_broad_hf</td>\n",
       "      <td>[0.3017578125, 0.27392578125]</td>\n",
       "      <td>[-0.96875, -1.30859375]</td>\n",
       "      <td>[0.283203125, 0.07080078125]</td>\n",
       "      <td>[0.5340414940762446, 0.4659585059237554]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180695</th>\n",
       "      <td>ffff059aea247f1dc7a09cfea55e00309b5b9a2e8cd9fc...</td>\n",
       "      <td>1000Tb 每秒传输10gb要多久才能完成</td>\n",
       "      <td>**1. 单位转换:**\\n\\n* 1000 Tb = 1000 * 1024 Gb = 1...</td>\n",
       "      <td>要计算传输 1000TB 数据以 10Gb/s 的速率需要多长时间，我们可以按以下步骤进行。...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-flash-exp-0827</td>\n",
       "      <td>chatgpt-4o-latest-20240903</td>\n",
       "      <td>unknown</td>\n",
       "      <td>train</td>\n",
       "      <td>current_comp</td>\n",
       "      <td>[0.701171875, 0.154296875]</td>\n",
       "      <td>[-1.6015625, -0.919921875]</td>\n",
       "      <td>[0.1044921875, 0.266357421875]</td>\n",
       "      <td>[0.985, 0.015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180696</th>\n",
       "      <td>ffff3da3bc7f4493b93b370bc0bab73e</td>\n",
       "      <td>I have submitted a paper on the topic of trust...</td>\n",
       "      <td>Dear Reviewers,\\n\\nThank you for your valuable...</td>\n",
       "      <td>In response to the question about the meaning ...</td>\n",
       "      <td>None</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>Nexusflow/Starling-LM-7B-beta</td>\n",
       "      <td>unknown</td>\n",
       "      <td>train</td>\n",
       "      <td>lmsys1m_vllm_gen</td>\n",
       "      <td>[0.34814453125, 0.259521484375]</td>\n",
       "      <td>[-1.166015625, -1.3828125]</td>\n",
       "      <td>[0.4423828125, -0.06363677978515625]</td>\n",
       "      <td>[0.5544351263355298, 0.4455648736644703]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180697</th>\n",
       "      <td>fffff4a6407e4bf9a231cfbb4f37f441</td>\n",
       "      <td>Quem ganhou a copa de 2022?</td>\n",
       "      <td>Desculpe, mas eu não tenho informações sobre u...</td>\n",
       "      <td>A resposta é que ainda não foi definido qual t...</td>\n",
       "      <td>None</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>train</td>\n",
       "      <td>lmsys1m_vllm_gen</td>\n",
       "      <td>[-0.24072265625, 0.94921875]</td>\n",
       "      <td>[-1.3984375, -1.1484375]</td>\n",
       "      <td>[-0.5390625, 1.0234375]</td>\n",
       "      <td>[0.25066048957472925, 0.7493395104252707]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180698 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       id  \\\n",
       "0       00007cff95d7f7974642a785aca248b0f26e60d3312fac...   \n",
       "1                        0000c48f341d489591e3bbeca1821926   \n",
       "2                        0000d826867d47dabd7d4b110beea2e9   \n",
       "3                        0000f23b2f8049ac9d92c28d4817381e   \n",
       "4       00010ed04b536f56ebe43eef1100c13906abea12bf9855...   \n",
       "...                                                   ...   \n",
       "180693                   fffe4d219bf14e50a7f29f64ee8a2fa0   \n",
       "180694                   fffe559651f241bdb473eb396ccdf042   \n",
       "180695  ffff059aea247f1dc7a09cfea55e00309b5b9a2e8cd9fc...   \n",
       "180696                   ffff3da3bc7f4493b93b370bc0bab73e   \n",
       "180697                   fffff4a6407e4bf9a231cfbb4f37f441   \n",
       "\n",
       "                                                   prompt  \\\n",
       "0                                      vieš po Slovensky?   \n",
       "1       ¿Cómo hago una petición Ajax en django con jQu...   \n",
       "2       You are now an expert educator. You have acces...   \n",
       "3       Please act my sick old grandma who would read ...   \n",
       "4       You will be given a piece of news. Analyze it ...   \n",
       "...                                                   ...   \n",
       "180693  puoi organizzare meglio questo documento latex...   \n",
       "180694                                           zxcvzxcv   \n",
       "180695                             1000Tb 每秒传输10gb要多久才能完成   \n",
       "180696  I have submitted a paper on the topic of trust...   \n",
       "180697                        Quem ganhou a copa de 2022?   \n",
       "\n",
       "                                               response_a  \\\n",
       "0        Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n",
       "1       Para hacer una petición Ajax en Django con jQu...   \n",
       "2       Cognitive bias is a tendency of the mind to fo...   \n",
       "3       Oh dear, it sounds like you're looking for a c...   \n",
       "4       Let's break down the news and analyze it accor...   \n",
       "...                                                   ...   \n",
       "180693  Certo, ecco il codice aggiornato con una maggi...   \n",
       "180694  It looks like you've typed a random string of ...   \n",
       "180695  **1. 单位转换:**\\n\\n* 1000 Tb = 1000 * 1024 Gb = 1...   \n",
       "180696  Dear Reviewers,\\n\\nThank you for your valuable...   \n",
       "180697  Desculpe, mas eu não tenho informações sobre u...   \n",
       "\n",
       "                                               response_b   winner  \\\n",
       "0       Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a   \n",
       "1       ¡Hola! ¡Claro que sí! Aquí te puedo ayudar con...     None   \n",
       "2       Cognitive Bias is a phenomenon that occurs whe...  model_a   \n",
       "3       Oh, my dear sweet child, come closer and snugg...     None   \n",
       "4       ```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...  model_a   \n",
       "...                                                   ...      ...   \n",
       "180693  \\documentclass{article}\\n\\n\\usepackage[utf8]{i...     None   \n",
       "180694  It seems like you've entered a string of rando...     None   \n",
       "180695  要计算传输 1000TB 数据以 10Gb/s 的速率需要多长时间，我们可以按以下步骤进行。...  model_a   \n",
       "180696  In response to the question about the meaning ...     None   \n",
       "180697  A resposta é que ainda não foi definido qual t...     None   \n",
       "\n",
       "                          model_a                             model_b  \\\n",
       "0                      o1-preview                  reka-core-20240904   \n",
       "1        Qwen/Qwen2-1.5B-Instruct                    llama-2-13b-chat   \n",
       "2                  fastchat-t5-3b                    RWKV-4-Raven-14B   \n",
       "3       MiniMaxAI/MiniMax-Text-01             deepseek-ai/DeepSeek-V3   \n",
       "4                  gemma-2-27b-it                gemini-1.5-flash-002   \n",
       "...                           ...                                 ...   \n",
       "180693                 vicuna-13b    microsoft/Phi-3-mini-4k-instruct   \n",
       "180694  MiniMaxAI/MiniMax-Text-01  meta-llama/Llama-3.1-405B-Instruct   \n",
       "180695  gemini-1.5-flash-exp-0827          chatgpt-4o-latest-20240903   \n",
       "180696                 vicuna-33b       Nexusflow/Starling-LM-7B-beta   \n",
       "180697                 vicuna-13b  mistralai/Mistral-7B-Instruct-v0.3   \n",
       "\n",
       "          language  split             source  \\\n",
       "0           Slovak  train       current_comp   \n",
       "1          Spanish  train   lmsys1m_vllm_gen   \n",
       "2          English  train          lmsys_33k   \n",
       "3          English  train  nicholas_broad_hf   \n",
       "4          Russian  valid       current_comp   \n",
       "...            ...    ...                ...   \n",
       "180693     Italian  train   lmsys1m_vllm_gen   \n",
       "180694      Danish  train  nicholas_broad_hf   \n",
       "180695     unknown  train       current_comp   \n",
       "180696     unknown  train   lmsys1m_vllm_gen   \n",
       "180697  Portuguese  train   lmsys1m_vllm_gen   \n",
       "\n",
       "                                      gemma9b                       gemma27b  \\\n",
       "0              [0.802734375, -0.159423828125]    [-0.728515625, -1.80859375]   \n",
       "1                [-0.40869140625, 0.94140625]       [-1.5546875, -0.9609375]   \n",
       "2       [0.3466033935546875, 0.2843017578125]     [-1.45703125, -0.95703125]   \n",
       "3               [0.16259765625, 0.3935546875]       [-1.421875, -1.01953125]   \n",
       "4               [1.048828125, -0.31005859375]  [-0.534423828125, -2.0859375]   \n",
       "...                                       ...                            ...   \n",
       "180693                   [-0.564453125, 1.25]      [-1.6484375, -0.80078125]   \n",
       "180694          [0.3017578125, 0.27392578125]        [-0.96875, -1.30859375]   \n",
       "180695             [0.701171875, 0.154296875]     [-1.6015625, -0.919921875]   \n",
       "180696        [0.34814453125, 0.259521484375]     [-1.166015625, -1.3828125]   \n",
       "180697           [-0.24072265625, 0.94921875]       [-1.3984375, -1.1484375]   \n",
       "\n",
       "                                        phi4  \\\n",
       "0                  [0.86328125, -0.49609375]   \n",
       "1              [-0.35595703125, 0.783203125]   \n",
       "2               [0.173828125, 0.28857421875]   \n",
       "3           [0.25439453125, 0.1134033203125]   \n",
       "4                           [1.0, -0.515625]   \n",
       "...                                      ...   \n",
       "180693                [-0.40625, 0.83984375]   \n",
       "180694          [0.283203125, 0.07080078125]   \n",
       "180695        [0.1044921875, 0.266357421875]   \n",
       "180696  [0.4423828125, -0.06363677978515625]   \n",
       "180697               [-0.5390625, 1.0234375]   \n",
       "\n",
       "                                      soft_labels  \n",
       "0                                  [0.985, 0.015]  \n",
       "1        [0.2412032986690225, 0.7587967013309777]  \n",
       "2                                  [0.985, 0.015]  \n",
       "3       [0.45704432372858816, 0.5429556762714118]  \n",
       "4                                  [0.985, 0.015]  \n",
       "...                                           ...  \n",
       "180693  [0.18557792051464522, 0.8144220794853547]  \n",
       "180694   [0.5340414940762446, 0.4659585059237554]  \n",
       "180695                             [0.985, 0.015]  \n",
       "180696   [0.5544351263355298, 0.4455648736644703]  \n",
       "180697  [0.25066048957472925, 0.7493395104252707]  \n",
       "\n",
       "[180698 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/pseudo_labeled_final_data/train_stage2_labeled_v1.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
