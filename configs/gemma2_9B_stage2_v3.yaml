debug: False
use_wandb: True
upload_to_hf_hub: True
sample_data: False
seed: -1

output_dir: ./artifacts
data_dir: ./data/pseudo_labeled_data_final


wandb_project_name: WSDM-Cup-Multilingual-Chatbot-Arena
experiment_name: gemma2_9b_stage2_v3
notes: gemma 2 9B stage 2 (ensemble logits from stage 1 models) [0.55 * gemma9b + 0.2 * gemma27b + 0.25 * phi4], hard labels for available smoothed and rest pseudo labeled with ensemble stage 1 logits, model init from original google checkpoint
train_file_name: train_stage2_labeled_v1.parquet

model:
  model_name: google/gemma-2-9b-it
  attn_implementation: flash_attention_2
  dtype: bfloat16
  quantize_to_4bit: true
  disable_attn_logit_softcapping: true
  

tokenizer:
  max_length: 2048
  truncation: True
  padding_side: left
  truncation_side: left
  pad_to_multiple_of: 16
  add_eos_token: true

lora:
  r: 64
  alpha: 64
  target_modules: all-linear
  dropout: 0
  bias: none
  modules_to_save: ['score']

optimizer:
  name: AdamW8bit
  head_lr: 2.0e-5
  lr: 1.0e-4


trainer_args:
  eval_strategy: epoch
  save_strategy: "no"
  logging_strategy: steps
  logging_first_step: True
  logging_steps: 1
  eval_on_start: false
  per_device_train_batch_size: 12
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 1
  learning_rate: ${optimizer.lr}
  num_train_epochs: 1
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01
  bf16: true
  tf32: true
  bf16_full_eval: true
  group_by_length: true
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  torch_compile: true
  use_liger_kernel: false
  report_to: "wandb"
  weight_decay: 0.01
  max_grad_norm: 1000.0
  ddp_find_unused_parameters: false

  